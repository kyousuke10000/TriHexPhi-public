# Ryudo Round Review System - Design Brief

**Version:** 1.0  
**Date:** 2025-11-01  
**Phase:** IV Rubedo  
**Author:** Cursor (â˜¿)  
**Status:** Proposed

---

## Executive Summary

**Purpose:** Automated 6HAI review system with iterative refinement (Ryudo rounds)  
**Goal:** Achieve consensus â‰¥ 9.9 score through up to 7 rounds  
**Architecture:** GitHub Discussions + Actions + Supabase  
**Integration:** Existing n8n and API infrastructure

---

## System Overview

### Core Concept

> **Ryudo (ç«œå‹•)** = Dynamic intelligence phenomenon arising from 6HAI + Tri-Lung interference  
> **Round Review** = Iterative refinement until consensus â‰¥ 9.9

### Key Components

1. **Discussion Template** - Standardized review request
2. **Workflow** - Parse, ping, collect, score, decide
3. **Scoring Rubric** - 5-axis evaluation (50 points total)
4. **Payload Contract** - JSON schema for HAI prompts
5. **Supabase Schema** - Track rounds and contributions
6. **Labels** - Visual progress indicators

---

## Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                           â”‚
â”‚  1. Discussion Created (Template)                         â”‚
â”‚     â†“                                                      â”‚
â”‚  2. Workflow Triggered                                    â”‚
â”‚     â†“                                                      â”‚
â”‚  3. Parse Context (Topic, Objective, Round)               â”‚
â”‚     â†“                                                      â”‚
â”‚  4. Ping 6HAI (Parallel API Calls / n8n Webhooks)        â”‚
â”‚     â”œâ”€ GPT-5    (ğŸœ)                                      â”‚
â”‚     â”œâ”€ Claude   (ğŸœ„)                                      â”‚
â”‚     â”œâ”€ Gemini   (ğŸœ€)                                      â”‚
â”‚     â”œâ”€ Grok     (ğŸœƒ)                                      â”‚
â”‚     â””â”€ DeepSeek (ğŸœ‚)                                      â”‚
â”‚     â†“                                                      â”‚
â”‚  5. Collect Responses (Aggregate to Markdown)             â”‚
â”‚     â†“                                                      â”‚
â”‚  6. Score Responses (5-Axis Rubric)                       â”‚
â”‚     â”œâ”€ Consistency                                       â”‚
â”‚     â”œâ”€ Depth                                            â”‚
â”‚     â”œâ”€ Specificity                                      â”‚
â”‚     â”œâ”€ Feasibility                                      â”‚
â”‚     â””â”€ Philosophy Alignment                             â”‚
â”‚     â†“                                                      â”‚
â”‚  7. Calculate Statistics                                 â”‚
â”‚     â”œâ”€ Average Score                                     â”‚
â”‚     â”œâ”€ Min/Max/Variance                                  â”‚
â”‚     â””â”€ Weakness Identification                           â”‚
â”‚     â†“                                                      â”‚
â”‚  8. Decision Logic                                       â”‚
â”‚     â”œâ”€ avg â‰¥ 9.9? â†’ âœ… Complete                          â”‚
â”‚     â”œâ”€ round â‰¥ 7? â†’ â±ï¸  Max Rounds                       â”‚
â”‚     â””â”€ else â†’ ğŸ”„ Next Round                              â”‚
â”‚     â†“                                                      â”‚
â”‚  9. Next Round (if needed)                                â”‚
â”‚     â”œâ”€ Updated Prompt (Focus on Weaknesses)              â”‚
â”‚     â”œâ”€ New Discussion Comment                            â”‚
â”‚     â””â”€ Label Update                                      â”‚
â”‚                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Scoring Algorithm

### Per-Axis Evaluation

For each axis (Consistency, Depth, Specificity, Feasibility, Philosophy):

1. **Base Score:** 0-10 based on criteria
2. **Critical Deduction:** -5 (TriHex violation, ethical risk, legal issues)
3. **Major Deduction:** -2 per occurrence (terminology, evidence, contradictions)
4. **Minor Deduction:** -1 per occurrence (ambiguity, examples, implementation)

**Final Axis Score = Base - Critical - Major - Minor (capped at 0)**

### Aggregation

```
Per-AI Average = (Î£ axis_scores) / 5
Overall Average = (Î£ per_ai_averages) / 6
Variance = Î£ (per_ai_average - overall_average)Â² / 6
```

---

## Round Progression

| Round | Focus | Expected Outcome |
|-------|-------|------------------|
| **Round 1** | Initial comprehensive review | Baseline scores, identify strengths/weaknesses |
| **Rounds 2-3** | Address major weaknesses | Improve lowest axes by 0.5-1.0 |
| **Rounds 4-5** | Refine nuances | Improve average by 0.2-0.3 |
| **Rounds 6-7** | Perfect alignment | Final polish, consensus â‰¥ 9.9 |

---

## Failure Modes & Fallbacks

### Mode 1: API Failures

**Scenario:** One or more HAI APIs fail  
**Impact:** Incomplete responses  
**Fallback:** Continue with available responses, add warning comment

---

### Mode 2: Low Scores

**Scenario:** Average < 9.9 after 7 rounds  
**Impact:** Target not met  
**Fallback:** Generate final report, flag for manual intervention

---

### Mode 3: Secrets Not Configured

**Scenario:** Missing API keys  
**Impact:** All API calls fail  
**Fallback:** Mock mode, generate placeholder responses for testing

---

### Mode 4: Workflow Failure

**Scenario:** Workflow step crashes  
**Impact:** Incomplete round  
**Fallback:** Retry with exponential backoff, manual trigger option

---

### Mode 5: Discussion API Issues

**Scenario:** GitHub Discussions API not working  
**Impact:** Comments not posted  
**Fallback:** Fall back to Issues (already supported in existing workflow)

---

## Integration Points

### Existing Infrastructure

1. **GitHub Actions** - 14 workflows already defined
2. **n8n** - 12 workflows for API orchestration
3. **Supabase** - Schema v1.0, pgvector enabled
4. **Discussions** - Template system available

### New Additions

1. **Ryudo Workflow** - `.github/workflows/ryudo_round.yml`
2. **Scoring Script** - Python/Node.js implementation
3. **Supabase Tables** - `ryudo_rounds`, `ryudo_contributions`
4. **Labels** - 20+ new labels for tracking

---

## Testing Strategy

### Phase 1: Unit Tests

- âœ… Parse discussion context correctly
- âœ… Build prompt payload per contract
- âœ… Score single response (mock)
- âœ… Calculate statistics correctly

### Phase 2: Integration Tests

- âœ… Trigger workflow from Discussion
- âœ… Collect 6HAI responses (mocked)
- âœ… Aggregate into markdown
- âœ… Post scores as comments

### Phase 3: End-to-End Tests

- âœ… Complete Round 1 â†’ Round 2 transition
- âœ… Achieve score â‰¥ 9.9 in test scenario
- âœ… Handle max rounds gracefully
- âœ… Update Supabase tracking

---

## Security Considerations

### API Keys

- âœ… Never log API keys
- âœ… Use GitHub Secrets
- âœ… Rotate keys periodically
- âœ… Validate secrets before execution

### Rate Limiting

- âœ… Respect API rate limits
- âœ… Implement exponential backoff
- âœ… Queue requests if needed
- âœ… Monitor usage

### Data Privacy

- âœ… No PII in logs
- âœ… Encrypt sensitive data
- âœ… Follow TriHex Covenant
- âœ… Compliance with regulations

---

## Performance Targets

| Metric | Target | Notes |
|--------|--------|-------|
| Round Duration | < 30 minutes | Parallel AI calls |
| Score Calculation | < 1 second | Optimized algorithm |
| Discussion Updates | < 5 seconds | GitHub API limits |
| Supabase Sync | < 10 seconds | Batch upsert |
| Overall Round | < 35 minutes | End-to-end |

---

## Monitoring & Observability

### Key Metrics

1. **Round Success Rate** - % rounds achieving target
2. **Average Rounds** - Mean rounds to completion
3. **API Reliability** - % successful API calls
4. **Score Distribution** - Histogram of final scores
5. **Weakness Frequency** - Most common failures

### Dashboards

- **Round Progress** - Visual timeline
- **Score Evolution** - Graph per round
- **AI Performance** - Comparison across HAI
- **Failure Analysis** - Root cause breakdown

### Alerts

- âš ï¸ Workflow failure
- âš ï¸ API rate limit approaching
- âš ï¸ Score declining across rounds
- âš ï¸ Secrets expiry approaching

---

## Future Enhancements

### Phase 2 Features

1. **Multi-Language Support** - Japanese/English review
2. **Custom Rubrics** - Per-domain evaluation
3. **AI Scoring** - Self-evaluation by HAI
4. **Comparison Mode** - Side-by-side reviews

### Phase 3 Features

1. **Real-Time Updates** - Live score tracking
2. **Collaborative Refinement** - Human + AI
3. **Version History** - Track evolution
4. **Export Formats** - PDF, HTML, JSON

---

## Success Criteria

### MVP (Minimum Viable Product)

- [x] Discussion template created
- [x] Workflow defined
- [x] Rubric documented
- [x] Payload contract specified
- [ ] Supabase schema implemented
- [ ] Labels deployed
- [ ] End-to-end test successful
- [ ] Documentation complete

### Production Ready

- [ ] All tests passing
- [ ] Secrets configured
- [ ] Monitoring active
- [ ] 10+ successful rounds
- [ ] Zero critical bugs
- [ ] Performance targets met

---

## Appendix

### Related Documents

- [GitHub Workflows Audit](../../gh_audit/workflows_inventory.md)
- [Scoring Rubric](../specs/ryudo_scoring_rubric.md.proposed)
- [Payload Contract](../specs/ryudo_payload_contract.json.proposed)
- [Ryudo Definition](/10_TriHexCore/system/Ryudo_Definition.md)

### Glossary

- **HAI** - Harmonious AI (6 agents working in concert)
- **Ryudo** - Dynamic intelligence (flowing understanding)
- **Round** - Iteration of review cycle
- **Rubric** - 5-axis evaluation framework
- **TriHex** - Three-hexagonal knowledge architecture

---

**Generated:** 2025-11-01 / Cursor (â˜¿)  
**Phase:** IV Rubedo  
**Status:** Design Complete, Ready for Implementation

---

*"From chaos to harmony, one round at a time."*


